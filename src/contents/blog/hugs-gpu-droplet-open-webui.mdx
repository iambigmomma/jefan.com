---
title: 'Deploying HuggingFace Generative AI Services on DigitalOcean GPU Droplet and Integrating with Open WebUI'
publishedAt: '2024-10-25'
description: 'Effortlessly deploy LLMs using HUGS on DigitalOcean GPU droplets without writing a single line of code. With the intuitive OpenWebUI, users can create custom ChatGPT-like services, setting up inference endpoints in just a few clicks. Experience the simplicity of building and customizing AI services, all through a clean, user-friendly interface that connects directly to your LLMs'
banner: 'hugs-banner'
tags: 'GPU,LLM,HuggingFace'
---

Deploying and running inference on LLMs typically presents AI engineers with several challenges, including optimizing model performance on diverse hardware, managing scalable infrastructure, and meeting compliance and security requirements. These complexities often increase development time and resource usage, especially for companies looking to shift from closed APIs to more customizable, open-source solutions.

[Hugging Face Generative AI Services (HUGS)](https://huggingface.co/docs/hugs) addresses these issues by providing a streamlined, API-compatible service optimized for various hardware configurations, enabling easy deployment of open models with minimal setup. This approach simplifies scalability, performance, and compliance management. DigitalOcean has partnered with HuggingFace to offer a [1-click deployment for HUGS](https://www.digitalocean.com/blog/one-click-models-on-do-powered-by-huggingface), making it easy to launch and run LLMs on DigitalOcean’s GPU-optimized infrastructure.

This guide will help you set up HUGS on a DigitalOcean GPU droplet, integrate it with Open WebUI, and explain why using DigitalOcean's infrastructure is simple and beneficial for LLM inference.

## Step-by-Step Tutorial

### Step 1: Create and Access Your GPU Droplet

1. **Navigate to the GPU Droplets Page**:  
   Log in to DigitalOcean and create a new droplet using the **HuggingFace HUGS 1-Click Deployment** option.

2. **Launch the Web Console**:  
    Once your droplet is set up, click on its name and then select **Launch Web Console**.
   ![web-console](https://res.cloudinary.com/iambigmomma/image/upload/v1685713949/blog/hugs-gpu-droplet-open-webui/web-console)

3. **(Optional)Login via SSH**:  
    If you’ve set up SSH, access the droplet through your terminal:

   ```bash
   ssh root@<your_droplet_public_IP>
   ```

   Note the **Message of the Day (MOTD)** that contains your bearer token for API access. You’ll need this later to integrate with Open WebUI.

![bear-token](https://res.cloudinary.com/iambigmomma/image/upload/v1685713949/blog/hugs-gpu-droplet-open-webui/bear-token)

### Step 2: Start 1-click model deployment with HUGS on DigitalOcean GPU droplet

After logging into your droplet, HuggingFace HUGS will automatically start. You can check the status of the Caddy service managing the inference API with:

```bash
sudo systemctl status caddy
```

Once deployed, allow 5 to 10 minutes for the model to warm up, though improvements to reduce this wait time are planned for future releases.

Check out this helpful walkthrough [video](https://www.youtube.com/watch?v=05t0JjX_plQ) available on YouTube.

### Step 3: Create a new droplet and deploy Open WebUI

Launch Open WebUI using Docker by running the following command on a seperate droplet:

```bash
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

Open WebUI will start on port 3000, accessible via `http://<your_droplet_ip>:3000`.

### Step 4: Integrate HUGS with Open WebUI

To connect [Open WebUI](https://openwebui.com/) with HUGS on DigitalOcean's GPU droplet:

![1-sign-up-open-webui](https://res.cloudinary.com/iambigmomma/image/upload/v1685713949/blog/hugs-gpu-droplet-open-webui/1-sign-up-open-webui)

1. **Open Admin Panel**:

   - In Open WebUI, click your **user icon** at the bottom left, then click **Admin Panel**.

![2-endpoint-setting](https://res.cloudinary.com/iambigmomma/image/upload/v1685713949/blog/hugs-gpu-droplet-open-webui/2-endpoint-setting)

2. **Go to Admin**:

   - Navigate to the **Setting** tab and select **Connections**.

3. **Set the Inference Endpoint**:

   - In the **API link field**, enter your droplet’s IP address followed by \`/v1\`. If a port is specified, include it, like `http://<your_droplet_ip>/v1`.
   - Use the API token from the **Message of the Day (MOTD)** for authentication.

4. **Verify Connection**:
   - Click **Verify Connection**. If successful, you will see a green light confirming the connection.
   - Once verified, the available model (e.g., `hfhugs/Meta-Llama-3.1-8B-Instruct`) will be automatically discovered from Workspace.

![3-model-discover](https://res.cloudinary.com/iambigmomma/image/upload/v1685713949/blog/hugs-gpu-droplet-open-webui/3-model-discover)

### Step 5: Start Chatting with the Model!

Congradulations! you’ve now successfully set up HUGS on a DigitalOcean GPU droplet and integrated it with Open WebUI. You’re ready to start interacting with your LLM and monitoring responses in real time!

- Ask questions like `What is DigitalOcean?`

![testing-example](https://res.cloudinary.com/iambigmomma/image/upload/v1685713949/blog/hugs-gpu-droplet-open-webui/testing-example)

- (Optional) View the Running Container’s Logs:
  Access the Web Console in your GPU droplet’s cloud panel. First, identify the container ID by running:

```bash
sudo docker ps
```

Then, stream the logs to monitor activity and verify responses in real time:

```bash
sudo docker logs <your-container-ID> -f
```

Check the logging from container while asking a follow-up question: `Does DigitalOcean offer object storage?`
![logging](https://res.cloudinary.com/iambigmomma/image/upload/v1685713949/blog/hugs-gpu-droplet-open-webui/logging)

## Final Thoughts: Why Use HUGS on DigitalOcean GPU Droplet?

### 1. **Ease of Deployment and Simplified Management**

With DigitalOcean’s one-click deployment, setting up HuggingFace HUGS is incredibly simple. You don’t need to configure hardware manually or worry about optimizing models for your infrastructure. HUGS and DigitalOcean take care of these aspects, allowing you to focus on scaling and customizing the deployment.

### 2. **Optimized Performance for Large-Scale Inference**

Using HUGS on DigitalOcean’s GPU droplets means you get a fully optimized backend. HuggingFace has pre-configured these models to run at maximum efficiency on GPUs, removing the need for manual optimization. This results in fast, reliable inference, without the need to worry about hardware tuning.

### 3. **Scalability and Flexibility**

DigitalOcean’s infrastructure is designed for scalability. You can deploy multiple GPU droplets and use global or regional load balancers to handle higher traffic or serve users across different regions. This setup provides fault tolerance, high availability, and seamless horizontal scaling.

---

By using HuggingFace HUGS on DigitalOcean GPU droplets, you not only benefit from high-performance LLM inference but also gain the flexibility to scale and manage the deployment effortlessly. This combination of optimized hardware, scalability, and simplicity makes DigitalOcean an excellent choice for production-level AI workloads.
